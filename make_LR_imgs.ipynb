{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import PIL\n",
    "import torch\n",
    "from PIL import Image,ImageFilter\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "# 生成卷积核和锚点\n",
    "def motion_kernel(length, angle):\n",
    "    EPS = np.finfo(float).eps\n",
    "    alpha = (angle - math.floor(angle / 180) * 180) / 180 * math.pi\n",
    "    half = length / 2\n",
    "    cosalpha = math.cos(alpha)\n",
    "    sinalpha = math.sin(alpha)\n",
    "    if cosalpha < 0:\n",
    "        xsign = -1\n",
    "    elif angle == 90:\n",
    "        xsign = 0\n",
    "    else:\n",
    "        xsign = 1\n",
    "    psfwdt = 1;\n",
    "    # 模糊核大小\n",
    "    sx = int(math.fabs(length * cosalpha + psfwdt * xsign - length * EPS))\n",
    "    sy = int(math.fabs(length * sinalpha + psfwdt - length * EPS))\n",
    "    psf1 = np.zeros((sy, sx))\n",
    "\n",
    "    # psf1是左上角的权值较大，越往右下角权值越小的核。\n",
    "    # 这时运动像是从右下角到左上角移动\n",
    "    for i in range(0, sy):\n",
    "        for j in range(0, sx):\n",
    "            psf1[i][j] = i * math.fabs(cosalpha) - j * sinalpha\n",
    "            rad = math.sqrt(i * i + j * j)\n",
    "            if rad >= half and math.fabs(psf1[i][j]) <= psfwdt:\n",
    "                temp = half - math.fabs((j + psf1[i][j] * sinalpha) / cosalpha)\n",
    "                psf1[i][j] = math.sqrt(psf1[i][j] * psf1[i][j] + temp * temp)\n",
    "            psf1[i][j] = psfwdt + EPS - math.fabs(psf1[i][j]);\n",
    "            if psf1[i][j] < 0:\n",
    "                psf1[i][j] = 0\n",
    "    # 运动方向是往左上运动，锚点在（0，0）\n",
    "    anchor = (0, 0)\n",
    "    # 运动方向是往右上角移动，锚点一个在右上角\n",
    "    # 同时，左右翻转核函数，使得越靠近锚点，权值越大\n",
    "    if angle < 90 and angle > 0:\n",
    "        psf1 = np.fliplr(psf1)\n",
    "        anchor = (psf1.shape[1] - 1, 0)\n",
    "    elif angle > -90 and angle < 0:  # 同理：往右下角移动\n",
    "        psf1 = np.flipud(psf1)\n",
    "        psf1 = np.fliplr(psf1)\n",
    "        anchor = (psf1.shape[1] - 1, psf1.shape[0] - 1)\n",
    "\n",
    "    elif anchor < -90:  # 同理：往左下角移动\n",
    "        psf1 = np.flipud(psf1)\n",
    "        anchor = (0, psf1.shape[0] - 1)\n",
    "\n",
    "    kernel = psf1 / psf1.sum()\n",
    "    return kernel, anchor\n",
    "\n",
    "def motion_blur(img,kernel, anchor):\n",
    "    motion_blur = cv2.filter2D(np.array(img), -1, kernel, anchor=anchor)\n",
    "    return PIL.Image.fromarray(motion_blur)\n",
    "\n",
    "IMG_EXTENSIONS = [\n",
    "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP','.tif',\n",
    "]\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "def HR2LR(img,motion_kernel=None,motion_anchor=None,gauss=None,scale=None):\n",
    "    if motion_kernel is not None and motion_anchor is not None:\n",
    "        img = motion_blur(img,motion_kernel,motion_anchor)\n",
    "    if gauss is not None:\n",
    "        img = img.filter(ImageFilter.GaussianBlur(gauss))\n",
    "    if scale is not None:\n",
    "        img = img.resize((int(img.size[0] / scale), int(img.size[1] / scale)), Image.ANTIALIAS)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save: /home/wcd/Projects/Pytorch-examples/fast_neural_style/images/test/foreman.tif\n",
      "save: /home/wcd/Projects/Pytorch-examples/fast_neural_style/images/test/comic.bmp\n",
      "save: /home/wcd/Projects/Pytorch-examples/fast_neural_style/images/test/starfish.tif\n",
      "save: /home/wcd/Projects/Pytorch-examples/fast_neural_style/images/test/timg.jpg\n",
      "save: /home/wcd/Projects/Pytorch-examples/fast_neural_style/images/test/butterfly.tif\n",
      "save: /home/wcd/Projects/Pytorch-examples/fast_neural_style/images/test/pepper.bmp\n",
      "save: /home/wcd/Projects/Pytorch-examples/fast_neural_style/images/test/man.bmp\n",
      "save: /home/wcd/Projects/Pytorch-examples/fast_neural_style/images/test/girl.tif\n",
      "save: /home/wcd/Projects/Pytorch-examples/fast_neural_style/images/test/flowers.bmp\n",
      "save: /home/wcd/Projects/Pytorch-examples/fast_neural_style/images/test/lena.tif\n",
      "save: /home/wcd/Projects/Pytorch-examples/fast_neural_style/images/test/zebra.bmp\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "motion_len = 10\n",
    "motion_angel = 66\n",
    "gauss = 5\n",
    "kernel,anchor = motion_kernel(motion_len, motion_angel)\n",
    "imgdir = \"/home/wcd/Projects/Pytorch-examples/fast_neural_style/images/style-images\"\n",
    "lrimgdir = \"/home/wcd/Projects/Pytorch-examples/fast_neural_style/images/test\"\n",
    "if not os.path.exists(lrimgdir):\n",
    "    os.mkdir(lrimgdir)\n",
    "for img in os.listdir(imgdir):\n",
    "    hrname = os.path.join(imgdir, img)\n",
    "    lrsavename = os.path.join(lrimgdir, img)\n",
    "    if is_image_file(img):\n",
    "        images.append(img)\n",
    "        hr = Image.open(hrname)\n",
    "        lr = HR2LR(hr,kernel, anchor,gauss)\n",
    "        lr.save(lrsavename)\n",
    "        print(\"save: \"+lrsavename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class LRNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LRNet, self).__init__()\n",
    "        # Initial convolution layers\n",
    "        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n",
    "        self.in1 = torch.nn.InstanceNorm2d(32, affine=True)\n",
    "        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
    "        self.in2 = torch.nn.InstanceNorm2d(64, affine=True)\n",
    "        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n",
    "        self.in3 = torch.nn.InstanceNorm2d(128, affine=True)\n",
    "        # Residual layers\n",
    "        self.res1 = ResidualBlock(128)\n",
    "        self.res2 = ResidualBlock(128)\n",
    "        self.res3 = ResidualBlock(128)\n",
    "        self.res4 = ResidualBlock(128)\n",
    "        self.res5 = ResidualBlock(128)\n",
    "        # Conv+PS upsampling Layers\n",
    "        self.ps1 = ConvPSLayer(128, 32, kernel_size=3, stride=1, upsample=2)\n",
    "        self.in4 = torch.nn.InstanceNorm2d(32, affine=True)\n",
    "        self.ps2 = ConvPSLayer(32, 8, kernel_size=3, stride=1, upsample=2)\n",
    "        self.in5 = torch.nn.InstanceNorm2d(8, affine=True)\n",
    "        self.conv4 = ConvLayer(8, 3, kernel_size=1, stride=1)\n",
    "        # Non-linearities\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.relu(self.in1(self.conv1(X)))\n",
    "        y = self.relu(self.in2(self.conv2(y)))\n",
    "        y = self.relu(self.in3(self.conv3(y)))\n",
    "        y = self.res1(y)\n",
    "        y = self.res2(y)\n",
    "        y = self.res3(y)\n",
    "        y = self.res4(y)\n",
    "        y = self.res5(y)\n",
    "        y = self.relu(self.in4(self.ps1(y)))\n",
    "        y = self.relu(self.in5(self.ps2(y)))\n",
    "        y = self.conv4(y)\n",
    "        return y\n",
    "    \n",
    "\n",
    "class ConvLayer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.reflection_pad(x)\n",
    "        out = self.conv2d(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    \"\"\"ResidualBlock\n",
    "    introduced in: https://arxiv.org/abs/1512.03385\n",
    "    recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.in1(self.conv1(x)))\n",
    "        out = self.in2(self.conv2(out))\n",
    "        out = out + residual\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpsampleConvLayer(torch.nn.Module):\n",
    "    \"\"\"UpsampleConvLayer\n",
    "    Upsamples the input and then does a convolution. This method gives better results\n",
    "    compared to ConvTranspose2d.\n",
    "    ref: http://distill.pub/2016/deconv-checkerboard/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
    "        super(UpsampleConvLayer, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        if upsample:\n",
    "            self.upsample_layer = torch.nn.UpsamplingNearest2d(scale_factor=upsample)\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_in = x\n",
    "        if self.upsample:\n",
    "            x_in = self.upsample_layer(x_in)\n",
    "        out = self.reflection_pad(x_in)\n",
    "        out = self.conv2d(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvPSLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    conv of (in,outxuxu,k,s)\n",
    "    follow by\n",
    "    pixel shuffle of scalling factor (u)\n",
    "    output is (bs out hxu wxu)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
    "        super(ConvPSLayer, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        if upsample:\n",
    "            self.pixelshuffle_layer = torch.nn.PixelShuffle(upsample)\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels*upsample^2, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.reflection_pad(x)\n",
    "        out = self.conv2d(out)\n",
    "        if self.upsample:\n",
    "            out = self.pixelshuffle_layer(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRNet\n",
      "ConvLayer\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ConvLayer' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-3c6a3d39c3e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclassname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Conv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mclassname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BatchNorm'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wcd/app/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 238\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConvLayer' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "lrnet = LRNet()\n",
    "import torch.nn.init as init\n",
    "lrnetchild = lrnet.modules()\n",
    "for i in lrnetchild:\n",
    "    classname = i.__class__.__name__\n",
    "    print(classname)\n",
    "    if classname.find('Conv') != -1:\n",
    "        i.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        i.weight.data.normal_(1.0, 0.02)\n",
    "        i.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'ndimension'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-92b6707ec960>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxavier_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/wcd/app/anaconda3/lib/python3.6/site-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mxavier_normal\u001b[0;34m(tensor, gain)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0mfan_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfan_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_calculate_fan_in_and_fan_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgain\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfan_in\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfan_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wcd/app/anaconda3/lib/python3.6/site-packages/torch/nn/init.py\u001b[0m in \u001b[0;36m_calculate_fan_in_and_fan_out\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_calculate_fan_in_and_fan_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0mdimensions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdimensions\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fan in and fan out can not be computed for tensor with less than 2 dimensions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'ndimension'"
     ]
    }
   ],
   "source": [
    "init.xavier_normal(lrnet.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.conv2d.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937a6fc8>\n",
      "conv1.conv2d.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937a6788>\n",
      "in1.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937a6a88>\n",
      "in1.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937a6908>\n",
      "conv2.conv2d.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937a6108>\n",
      "conv2.conv2d.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb93776d08>\n",
      "in2.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb93776c88>\n",
      "in2.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb93776c48>\n",
      "conv3.conv2d.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937b6288>\n",
      "conv3.conv2d.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937b6648>\n",
      "in3.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937b6448>\n",
      "in3.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937b6608>\n",
      "res1.conv1.conv2d.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937b6dc8>\n",
      "res1.conv1.conv2d.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937b6e08>\n",
      "res1.in1.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937bb048>\n",
      "res1.in1.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937bb188>\n",
      "res1.conv2.conv2d.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937bb508>\n",
      "res1.conv2.conv2d.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937bb548>\n",
      "res1.in2.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937bb888>\n",
      "res1.in2.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937bb8c8>\n",
      "res2.conv1.conv2d.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937c0048>\n",
      "res2.conv1.conv2d.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937c0088>\n",
      "res2.in1.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937c0288>\n",
      "res2.in1.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937c03c8>\n",
      "res2.conv2.conv2d.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937c0748>\n",
      "res2.conv2.conv2d.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937c0788>\n",
      "res2.in2.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937c0ac8>\n",
      "res2.in2.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937c0b08>\n",
      "res3.conv1.conv2d.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb93746288>\n",
      "res3.conv1.conv2d.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937462c8>\n",
      "res3.in1.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937464c8>\n",
      "res3.in1.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb93746608>\n",
      "res3.conv2.conv2d.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb93746788>\n",
      "res3.conv2.conv2d.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937468c8>\n",
      "res3.in2.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb93746d08>\n",
      "res3.in2.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb93746d48>\n",
      "res4.conv1.conv2d.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb9374a2c8>\n",
      "res4.conv1.conv2d.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb9374a408>\n",
      "res4.in1.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb9374a388>\n",
      "res4.in1.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb9374a3c8>\n",
      "res4.conv2.conv2d.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb9374a948>\n",
      "res4.conv2.conv2d.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb9374abc8>\n",
      "res4.in2.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb9374ab48>\n",
      "res4.in2.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb9374ab88>\n",
      "res5.conv1.conv2d.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb93769548>\n",
      "res5.conv1.conv2d.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb93769588>\n",
      "res5.in1.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb93769708>\n",
      "res5.in1.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb93769848>\n",
      "res5.conv2.conv2d.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb93749388>\n",
      "res5.conv2.conv2d.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937490c8>\n",
      "res5.in2.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb9377f248>\n",
      "res5.in2.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb9377f288>\n",
      "ps1.conv2d.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937c5088>\n",
      "ps1.conv2d.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937c50c8>\n",
      "in4.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937c5248>\n",
      "in4.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937c5288>\n",
      "ps2.conv2d.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937c5848>\n",
      "ps2.conv2d.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937c5888>\n",
      "in5.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937c5a08>\n",
      "in5.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937c5a48>\n",
      "conv4.conv2d.weight\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937c5ec8>\n",
      "conv4.conv2d.bias\n",
      "<built-in method size of torch.FloatTensor object at 0x7fbb937c5f08>\n"
     ]
    }
   ],
   "source": [
    "for name, param in lrnet.named_parameters():\n",
    "    print(name)\n",
    "    print(param.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LRNet'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrnet.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRNet\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "classname = LRNet().__class__.__name__\n",
    "print(classname)\n",
    "print(classname.find('LRNet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'filters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-3b0d4affc10a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m IMG_EXTENSIONS = [\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'filters'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image,ImageFilter\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import os.path\n",
    "import filters\n",
    "\n",
    "IMG_EXTENSIONS = [\n",
    "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n",
    "]\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "#load img from floder using filename\n",
    "def load_HR_image(filename, size=None, mode='Y'):\n",
    "    if mode is \"Y\":\n",
    "        y, cb, cr = Image.open(filename).convert('L').split()\n",
    "        img = y\n",
    "    elif mode is 'RGB':\n",
    "        img = Image.open(filename).convert('RGB')\n",
    "    elif mode is 'YCbCr':\n",
    "        img = Image.open(filename).convert('YCbCr')\n",
    "    if size is not None:\n",
    "        img = img.resize((size, size), Image.ANTIALIAS)\n",
    "    return img\n",
    "\n",
    "def load_LR_image(filename, motion_len=0,motion_angel=0,gauss=0,size=None,scale=None, mode='Y'):\n",
    "    if mode is \"Y\":\n",
    "        y, cb, cr = Image.open(filename).convert('L').split()\n",
    "        img = y\n",
    "    elif mode is 'RGB':\n",
    "        img = Image.open(filename).convert('RGB')\n",
    "    elif mode is 'YCbCr':\n",
    "        img = Image.open(filename).convert('YCbCr')\n",
    "    if size is not None:\n",
    "        img = img.resize((size, size), Image.ANTIALIAS)\n",
    "    elif scale is not None:\n",
    "        img = img.resize((int(size / scale), int(size / scale)), Image.ANTIALIAS)\n",
    "    if motion_len is not 0 and motion_angel is not 0:\n",
    "        motion_kernel,motion_anchor = filters.motion_kernel(motion_len,motion_angel)\n",
    "        img = filters.motion_blur(img,motion_kernel,motion_anchor)\n",
    "    if gauss is not 0:\n",
    "        img = img.filter(ImageFilter.GaussianBlur(gauss))\n",
    "\n",
    "    return img\n",
    "\n",
    "#Load content img\n",
    "def default_loader(path):\n",
    "    l = Image.open(path).convert('L')\n",
    "    return Image.merge(\"RGB\", (l, l, l))\n",
    "\n",
    "#transfer HR PIL img to LR PIL img\n",
    "def HR2LR(img,motion_kernel=None,motion_anchor=None,gauss=None,scale=None):\n",
    "    if motion_kernel is not None and motion_anchor is not None:\n",
    "        img = filters.motion_blur(img,motion_kernel,motion_anchor)\n",
    "    if gauss is not None:\n",
    "        img = img.filter(ImageFilter.GaussianBlur(gauss))\n",
    "    if scale is not None:\n",
    "        img = img.resize((int(img.size[0] / scale), int(img.size[1] / scale)), Image.ANTIALIAS)\n",
    "    return img\n",
    "\n",
    "class trainingFolder(data.Dataset):\n",
    "\n",
    "    def __init__(self, root, HR_size, LR_scale=None,\n",
    "                 motion_len=None,motion_angel=None,gauss=None,\n",
    "                 transform=None, target_transform=None,loader=load_HR_image,mode='Y'):\n",
    "        super(trainingFolder,self).__init__()\n",
    "        self.loader = loader\n",
    "        if motion_len is not None and motion_angel is not None:\n",
    "            self.motion_kernel, self.motion_anchor = filters.motion_kernel(motion_len, motion_angel)\n",
    "        self.gauss = gauss\n",
    "        self.HR_size = HR_size\n",
    "        self.LR_scale = LR_scale\n",
    "        self.mode=mode\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.imgs = make_dataset(root)\n",
    "        if len(self.imgs ) == 0:\n",
    "            raise (RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\"\n",
    "                                                                             \"Supported image extensions are: \" + \",\".join(\n",
    "                IMG_EXTENSIONS)))\n",
    "    def __getitem__(self, index):\n",
    "        path = self.imgs[index]\n",
    "        #training img--content\n",
    "        img = self.loader(path,self.HR_size,mode=self.mode)\n",
    "        #target img--reference\n",
    "        target = HR2LR(img,self.motion_kernel,self.motion_anchor,self.gauss,self.LR_scale)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "def make_dataset(dir):\n",
    "    images = []\n",
    "    for target in os.listdir(dir):\n",
    "        fname = os.path.join(dir, target)\n",
    "        if is_image_file(fname):\n",
    "            images.append(fname)\n",
    "    return images\n",
    "\n",
    "def save_image(filename, data):\n",
    "    img = data.clone().clamp(0, 255).numpy()\n",
    "    img = img.transpose(1, 2, 0).astype(\"uint8\")\n",
    "    img = Image.fromarray(img)\n",
    "    img.save(filename)\n",
    "\n",
    "\n",
    "def gram_matrix(y):\n",
    "    (b, ch, h, w) = y.size()\n",
    "    features = y.view(b, ch, w * h)\n",
    "    features_t = features.transpose(1, 2)\n",
    "    gram = features.bmm(features_t) / (ch * h * w)\n",
    "    return gram\n",
    "\n",
    "\n",
    "def normalize_batch(batch):\n",
    "    # normalize using imagenet mean and std\n",
    "    mean = batch.data.new(batch.data.size())\n",
    "    std = batch.data.new(batch.data.size())\n",
    "    mean[:, 0, :, :] = 0.485\n",
    "    mean[:, 1, :, :] = 0.456\n",
    "    mean[:, 2, :, :] = 0.406\n",
    "    std[:, 0, :, :] = 0.229\n",
    "    std[:, 1, :, :] = 0.224\n",
    "    std[:, 2, :, :] = 0.225\n",
    "    batch = torch.div(batch, 255.0)\n",
    "    batch -= Variable(mean)\n",
    "    batch /= Variable(std)\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nerual_style'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-984f1911630f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnerual_style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mLRnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformerNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisdom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nerual_style'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import nerual_style\n",
    "from LRnet import TransformerNet\n",
    "import visdom\n",
    "\n",
    "#learn LR ,but reference is itself\n",
    "def check_paths(args):\n",
    "    try:\n",
    "        if not os.path.exists(args.save_model_dir):\n",
    "            os.makedirs(args.save_model_dir)\n",
    "        if args.checkpoint_model_dir is not None and not (os.path.exists(args.checkpoint_model_dir)):\n",
    "            os.makedirs(args.checkpoint_model_dir)\n",
    "    except OSError as e:\n",
    "        print(e)\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "\n",
    "args.subcommand = \"train\"\n",
    "args.cuda = 1\n",
    "args.save_model_dir = \"ckpt\"\n",
    "args.dataset = '/home/wcd/LinkToMyLib/Datas/train2014'\n",
    "args.style_image = \"/home/wcd/Projects/Pytorch-examples/fast_neural_style/images/style-images/comic.bmp\"\n",
    "args.epochs = 2\n",
    "args.batch_size = 8\n",
    "args.lr = 1e-3\n",
    "args.content_weight = 1e3\n",
    "args.style_weight = 1e10\n",
    "args.checkpoint_model_dir = \"ckpt\"\n",
    "args.checkpoint_interval = 2000\n",
    "args.log_interval = 200/args.batch_size\n",
    "args.plot_interval = 64\n",
    "args.seed = 42\n",
    "args.image_size = 128\n",
    "args.style_size = None\n",
    "args.HR_size = 128\n",
    "args.LR_scale = 1\n",
    "args.motion_len = 10\n",
    "args.motion_angel = 66\n",
    "args.gauss = 5\n",
    "mode = args.mode = 'YCbCr'\n",
    "#args.subcommand = \"eval\"\n",
    "args.model = \"/home/wcd/Projects/Pytorch-examples/fast_neural_style/neural_style/ckpt/ckpt_epoch_0_batch_id_6000.pth\"\n",
    "args.content_image = \"/home/wcd/Projects/Pytorch-examples/fast_neural_style/images/style-images/flowers.bmp\"\n",
    "args.content_scale = 2\n",
    "args.output_image = \"/home/wcd/Projects/Pytorch-examples/fast_neural_style/images/output-images/flowers-6000.jpg\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
