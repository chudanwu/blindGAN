<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

## 问题建模
把去模糊和超分问题一起考虑，可以得到如下建模：
$$ 已知：退化图 L；未知：退化核 k；下采样操作：D；斯噪声 n；高清图 H；待求解：重建图 X。$$
- 在blind-deblur中，他们的关系是：
$$ H*k+n=L $$
- 在super-resolution中，他们的关系是：
$$ D \circ (H*k)=L$$


### 超分重建````
- SCSR (08cvpr稀疏编码):
$$ argmin_X ||X-H|| \quad s.t.\quad D \circ (H*k)=L $$
- SRCNN (15cvpr深度卷积)：
$$ argmin_{\theta}MSE(X_\theta-H) $$
- SRGAN (17cvpr深度生成对抗)：
$$ argmin_{\theta} ( MSE(\phi(X_\theta )-\phi(H)) + 对抗loss)$$

均在文中明确指出，目标是使恢复的图像X逼近H。

### 盲去模糊
- DL blur kernel估计（2016IP）：

    这篇文章中，由于估计的是核的参数，因此最小化的是参数和参数groundTruth的误差。
- text deblur by L0 正则化（2017pami）：
$$ argmin_{X,k} ||X*k-L||^2_2 + \lambda l_0(X) + \gamma l_2(k)$$

    迭代优化求解X,k。其中，求k的时候，用的是（X的梯度*k）和（L的梯度）。这里之所以和SR反过来，我觉得是因为这是个无监督的方法(用不上H)，所以只能和L比较。
- direct text deblur：
    
    作者明确指出，由于真实模糊比模糊核卷积更复杂,所以比起模拟模糊过程，模拟去模糊过程更实际：
    $$ argmin_\theta MSE(X-H) + 0.0005 ||\theta||^2_2$$


## Loss

$$单个样本。输入：x,标签：y,输出：F(x),网络参数：\theta,特征：\phi(x)$$
下面的loss都是适用于batch的，即对一个batch，采取了均值loss。

|名称|表达式|作用|适用情况|
|:--|:-----|:--|:-------|
|L1 loss|$$ \|F(x)-y\|_1 $$|输出大体上逼近y|普适|
|mse loss|$$ \frac{1}{CHW}\|F(x)-y\|^2_2 $$|输出大体上平滑的逼近y|普适|
|perceptual loss|$$ \|\phi(F(x))-\phi(y)\|_2 $$|输出的内容逼近y的内容|语义信息|
|adversaral loss|$$ argmin_{G}max_{D}  [\log D(x,y)+\log(1-D(x,F(x)))]$$|判别网络无法区分输出和y是否condition的区别在于D是两个输入还是一个|在判别网络中|
|adversaral loss|$$ argmin_{G}max_{D}  [\| D(x,y)\|_2+\|1-D(x,F(x))\|_2]$$|替换negative log likelihood为least quare，
|gram loss|$$ \|gram(\phi(F(x)))-gram( \phi(y))\|_2 $$ |输出的纹理逼近y的纹理|风格转换中不含语义|
|Lp loss|$$ \|F(x)-y\|_p $$|在L1与L2中折中|比L1、L2效果都要好时|
|identity loss|$$ \|x-F(x)\|_1 $$|当输入本身就是输出同一类时，输入=输出|有助于在transfer任务中保留颜色|
|TotalVariant loss|$$\|\nabla _H F(x)\|_1+\| \nabla _W F(x)\|_1$$ |平滑输出，削弱artifact|去噪中常见|
|l1/l2 piror|$$ \|F(x)\|_1 \quad or \quad \|F(x)\|^2_2 $$|不知道有没有用
|l2 reg|$$ \|\theta\|_1  \quad or \quad \|\theta\|^2_2 $$|参数稀疏/用小参数可以帮助收敛|非必须



## 结构
|网络结构|作用|适用情况|出自|
|:------|:--|:-------|:--|
|瓶颈结构|减少参数|图片较大，追求速度，中间层数较多|>-< 形状流行于 fast-style-transfer|
|skip connection|有选择性的保留输入|重建精细的任务|出自 U-net，流行于后来的SR、GAN任务|
|resisual learning|只学习残差，帮助收敛|普遍适用|已广泛应用于CNN|
|IN/BN|降低特征表达学习难度，帮助收敛|？？？因为有文章说可以去掉|已广泛应用于CNN|
|GAN|获得一个非设计的loss：对抗loss|设计的loss效果不好|流行于生成式任务|
|buffer判别式|在GAN训练D时，不选用最新G输出而是保留历史G输出作为“fake”|避免模型参数震荡|来自pix2pix的引用文章|
|deconv|有upconv，transconv，subpixel三种|第一种效果最好也最慢，第三种最快|第一种SRCNN在用，第三种来自ESPCN，第二种fast-style-transfer系列在用|
|patchGAN|判别器输出非1维而是dxd矩阵，即感受域只对应输入一部分而非全部，加速训练|假设像素符合马尔科夫特性|pix2pix引用的文章|


## 实验及对应结果

1. 非盲去模糊可行性实验：

    参数： 
    motion blur:len=7,;length=

    数据集：
    training set=COCO 2014;test set=9 img from yang
    
    网络结构：
    瓶颈结构+skipconnect+IN+res learning+ 8 res_block +subpixel。

    loss：
    L1 loss。其他reg项已经实现但还没用

    优化方法：
    adam

    核参数：运动=0.gauss=1.7 模型参数：运=0.gauss=1.7


|img-name |blurPSNR | +PSNR |
|:--------|--------:|------:|
|comic.bmp	 |-1.876827	|32.462749|
|pepper.bmp	 |2.274908	|33.841355|
|man.bmp	 |-1.232231	|33.332980|
|flowers.bmp	 |-0.823303	|35.407222|
|zebra.bmp	 |-2.251888	|36.726342|



2. pair blindGAN ：
    
    loss：

-[X]
    $$ 
    ldentity\ loss:\ argmin_G = E|| G(H)-\delta ||_1
    $$
-[ ]
    $$ 
    cycle\ loss:\ argmin_G = E|| H*G(L)-L ||_1
    $$
-[X]
    $$
    GAN\ loss:\ argmin_Gmax_D = E[\log(D(L,k))] + E[1-\log(D(L,G(x)))]
    $$
-[X]
    $$
    generate \ loss:\ argmin_G = E||G(L)-k||_1
    $$

    结构：

    G：

        scaling：1x116x116->1x1x29 √
        resblock：conv+norm+relu+dropout √
        lastactivate：tanh/sigmoid
    
    D：

        fully convolution layer: conv+norm +leakyrelu(sigmoid for last 0-1 output) √

    预处理：
    
        G的输入random crop
        D用上history pool √
        注意使用detach避免不必要的梯度反向 √




3. encoder-decoderSRnet
疑问：好像不需要p？但是p相关的weight是有值的
seperate training：
-----------------------------------------------
|no.	|gauss	|motion_len	|motion_ang	|avg PSNR inc	|avg PSNR	|
|-------|-------|---|---|-------|-------|
|  1	|0.0	|3	|10	|2.68	|32.56	|
|  2	|0.0	|3	|20	|2.72	|31.92	|
|  3	|0.0	|3	|30	|2.96	|32.38	|
|  4	|0.0	|3	|40	|2.56	|31.84	|
|  5	|0.0	|3	|50	|2.65	|31.98	|
|  6	|0.0	|3	|60	|3.31	|32.87	|
|  7	|0.0	|3	|70	|3.14	|32.59	|
|  8	|0.0	|3	|80	|3.28	|33.46	|
|  9	|0.0	|4	|10	|2.89	|31.71	|
| 10	|0.0	|4	|20	|3.35	|30.86	|
| 11	|0.0	|4	|30	|3.91	|31.60	|
| 12	|0.0	|4	|40	|3.68	|31.22	|
| 13	|0.0	|4	|50	|3.97	|31.56	|
| 14	|0.0	|4	|60	|4.55	|32.37	|
| 15	|0.0	|4	|70	|4.33	|32.06	|
| 16	|0.0	|4	|80	|3.77	|32.87	|
| 17	|0.0	|5	|10	|3.39	|31.05	|
| 18	|0.0	|5	|20	|4.31	|30.77	|
| 19	|0.0	|5	|30	|4.53	|31.02	|
| 20	|0.0	|5	|40	|4.84	|31.22	|
| 21	|0.0	|5	|50	|4.99	|31.42	|
| 22	|0.0	|5	|60	|4.88	|31.49	|
| 23	|0.0	|5	|70	|5.14	|31.80	|
| 24	|0.0	|5	|80	|4.60	|32.49	|
| 25	|0.0	|6	|10	|2.97	|28.49	|
| 26	|0.0	|6	|20	|4.33	|29.81	|
| 27	|0.0	|6	|30	|4.89	|30.20	|
| 28	|0.0	|6	|40	|5.28	|30.81	|
| 29	|0.0	|6	|50	|5.45	|31.03	|
| 30	|0.0	|6	|60	|5.43	|30.86	|
| 31	|0.0	|6	|70	|5.22	|30.88	|
| 32	|0.0	|6	|80	|4.30	|30.05	|
| 33	|0.0	|7	|10	|2.03	|26.77	|
| 34	|0.0	|7	|20	|3.57	|28.30	|
| 35	|0.0	|7	|30	|4.23	|28.92	|
| 36	|0.0	|7	|40	|4.88	|29.66	|
| 37	|0.0	|7	|50	|5.23	|30.06	|
| 38	|0.0	|7	|60	|5.32	|30.12	|
| 39	|0.0	|7	|70	|4.94	|29.84	|
| 40	|0.0	|7	|80	|3.26	|28.20	|
| 41	|0.0	|8	|10	|1.14	|25.25	|
| 42	|0.0	|8	|20	|2.22	|26.16	|
| 43	|0.0	|8	|30	|3.45	|27.51	|
| 44	|0.0	|8	|40	|3.67	|27.93	|
| 45	|0.0	|8	|50	|4.17	|28.47	|
| 46	|0.0	|8	|60	|4.58	|28.76	|
| 47	|0.0	|8	|70	|3.27	|27.38	|
| 48	|0.0	|8	|80	|1.82	|26.13	|
| 49	|0.0	|0	|0	|-9957.54	|42.46	|
| 50	|0.3	|0	|0	|-15.81	|42.69	|
| 51	|0.7	|0	|0	|2.21	|36.43	|
| 52	|1.0	|0	|0	|3.14	|33.89	|
| 53	|1.3	|0	|0	|3.14	|32.29	|
| 54	|1.7	|0	|0	|4.22	|32.77	|
| 55	|2.0	|0	|0	|4.81	|32.95	|
| 56	|2.3	|0	|0	|4.81	|32.62	|
| 57	|2.7	|0	|0	|4.88	|32.64	|
| 58	|3.0	|0	|0	|4.78	|32.42	|

combine training：
-----------------------------------------------
|no.	|gauss	|motion_len	|motion_ang	|avg PSNR inc	|avg PSNR	|
|-------|-------|---|---|-------|-------|
|  1	|0.0	|3	|10	|7.89	|37.77	|
|  2	|0.0	|3	|20	|8.68	|37.88	|
|  3	|0.0	|3	|30	|8.72	|38.14	|
|  4	|0.0	|3	|40	|8.56	|37.84	|
|  5	|0.0	|3	|50	|8.49	|37.82	|
|  6	|0.0	|3	|60	|8.64	|38.20	|
|  7	|0.0	|3	|70	|8.42	|37.87	|
|  8	|0.0	|3	|80	|7.27	|37.46	|
|  9	|0.0	|4	|10	|7.80	|36.61	|
| 10	|0.0	|4	|20	|9.15	|36.67	|
| 11	|0.0	|4	|30	|8.96	|36.65	|
| 12	|0.0	|4	|40	|9.80	|37.34	|
| 13	|0.0	|4	|50	|9.56	|37.15	|
| 14	|0.0	|4	|60	|8.59	|36.41	|
| 15	|0.0	|4	|70	|9.01	|36.75	|
| 16	|0.0	|4	|80	|7.79	|36.89	|
| 17	|0.0	|5	|10	|8.73	|36.39	|
| 18	|0.0	|5	|20	|9.85	|36.31	|
| 19	|0.0	|5	|30	|9.58	|36.06	|
| 20	|0.0	|5	|40	|9.82	|36.20	|
| 21	|0.0	|5	|50	|9.99	|36.41	|
| 22	|0.0	|5	|60	|9.14	|35.75	|
| 23	|0.0	|5	|70	|9.49	|36.15	|
| 24	|0.0	|5	|80	|8.91	|36.80	|
| 25	|0.0	|6	|10	|9.17	|34.69	|
| 26	|0.0	|6	|20	|8.98	|34.46	|
| 27	|0.0	|6	|30	|9.31	|34.62	|
| 28	|0.0	|6	|40	|9.48	|35.02	|
| 29	|0.0	|6	|50	|9.75	|35.33	|
| 30	|0.0	|6	|60	|9.60	|35.03	|
| 31	|0.0	|6	|70	|8.84	|34.50	|
| 32	|0.0	|6	|80	|9.14	|34.89	|
| 33	|0.0	|7	|10	|9.69	|34.43	|
| 34	|0.0	|7	|20	|9.72	|34.45	|
| 35	|0.0	|7	|30	|9.59	|34.28	|
| 36	|0.0	|7	|40	|10.02	|34.80	|
| 37	|0.0	|7	|50	|9.75	|34.58	|
| 38	|0.0	|7	|60	|9.69	|34.50	|
| 39	|0.0	|7	|70	|9.56	|34.47	|
| 40	|0.0	|7	|80	|9.40	|34.34	|
| 41	|0.0	|8	|10	|6.22	|30.33	|
| 42	|0.0	|8	|20	|7.04	|30.98	|
| 43	|0.0	|8	|30	|8.69	|32.75	|
| 44	|0.0	|8	|40	|8.50	|32.75	|
| 45	|0.0	|8	|50	|8.96	|33.26	|
| 46	|0.0	|8	|60	|8.78	|32.96	|
| 47	|0.0	|8	|70	|6.83	|30.95	|
| 48	|0.0	|8	|80	|5.80	|30.10	|
| 49	|0.0	|0	|0	|-9954.93	|45.07	|
| 50	|0.3	|0	|0	|-13.22	|45.28	|
| 51	|0.7	|0	|0	|3.33	|37.55	|
| 52	|1.0	|0	|0	|4.64	|35.39	|
| 53	|1.3	|0	|0	|4.79	|33.95	|
| 54	|1.7	|0	|0	|6.31	|34.86	|
| 55	|2.0	|0	|0	|6.89	|35.04	|
| 56	|2.3	|0	|0	|6.81	|34.61	|
| 57	|2.7	|0	|0	|7.07	|34.84	|
| 58	|3.0	|0	|0	|6.92	|34.55	|
